---
title: "STAMP Quiz Diagnostic F23-W24"
subtitle: "Noah Lee"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---


```{r setup}
#| include: false
library(tidyverse)
library(tidymodels)
library(naniar)    # For visualizing missing data
library(dplyr)
library(ggformula)
library(ggplot2) 
library(GGally)

library(lubridate)  # For date handling
library(ranger)     # Fast implementation of Random Forest 
library(workflows)  # To create modeling workflows
library(recipes)    # For feature engineering
tidymodels_prefer(quiet = TRUE) 
```

```{r}
df <- read.csv('../Data/Novice.csv')
```

```{r}
# Initial Inspection
print("Initial Inspection:")
str(df)
head(df)
```

```{r}
# Apply na_if only to character and factor columns
df <- df %>% 
  mutate(across(where(is.character), ~na_if(., ""))) %>%
  mutate(across(where(is.character), ~na_if(., " "))) %>%
  mutate(across(where(is.factor), ~na_if(as.character(.), "") %>% factor()))

```


```{r}
cat("\nMissing values per column:\n")
colSums(is.na(df))
```
```{r}
vis_miss(df) 
```



```{r}
compare_match_length_by_missingness <- function(df, column_name) {
  # Create binary indicator for missingness
  is_missing <- is.na(df[[column_name]])
  
  length_when_missing <- df[is_missing, "Match.Length"] 
  length_when_present <- df[!is_missing, "Match.Length"]
  
  # Check if we have enough data
  if (sum(is_missing) == 0 || sum(!is_missing) == 0) {
    return(data.frame(
      column = column_name,
      missing_count = sum(is_missing),
      present_count = sum(!is_missing),
      mean_when_missing = NA,
      mean_when_present = NA,
      difference = NA,
      percent_difference = NA,
      p_value = NA,
      significant = NA
    ))
  }
  
  # Simple check to make sure we have data to work with
  print(paste("Analyzing column:", column_name))
  print(paste("Missing values:", sum(is_missing)))
  print(paste("Present values:", sum(!is_missing)))
  
  # Calculate basic statistics (safely)
  mean_when_missing <- if(length(length_when_missing) > 0) mean(length_when_missing, na.rm = TRUE) else NA
  mean_when_present <- if(length(length_when_present) > 0) mean(length_when_present, na.rm = TRUE) else NA
  
  # Calculate difference and percent difference safely
  diff_value <- mean_when_missing - mean_when_present
  percent_diff <- if(!is.na(mean_when_present) && mean_when_present != 0) {
    (diff_value / mean_when_present) * 100
  } else {
    NA
  }
  
  # Only perform t-test if we have enough data
  if(length(length_when_missing) > 1 && length(length_when_present) > 1) {
    t_test_result <- tryCatch({
      t.test(length_when_missing, length_when_present, var.equal = FALSE)
    }, error = function(e) {
      return(list(p.value = NA))
    })
    p_value <- t_test_result$p.value
  } else {
    p_value <- NA
  }
  
  return(data.frame(
    column = column_name,
    missing_count = sum(is_missing),
    present_count = sum(!is_missing),
    mean_when_missing = mean_when_missing,
    mean_when_present = mean_when_present,
    difference = diff_value,
    percent_difference = percent_diff,
    p_value = p_value,
    significant = !is.na(p_value) && p_value < 0.05
  ))
}

test_column <- names(df)[which.max(colSums(is.na(df)))]
test_result <- compare_match_length_by_missingness(df, test_column)
print(test_result)

columns_to_check <- names(df)[colSums(is.na(df)) > 0]
results <- list()

for (col in columns_to_check) {
  result <- compare_match_length_by_missingness(df, col)
  if (!all(is.na(result))) {  # Only keep meaningful results
    results[[col]] <- result
  }
}
```


```{r}
# Combine results and check structure
if (length(results) > 0) {
  results_df <- bind_rows(results)
  print(dim(results_df))
  print(head(results_df))
  
  # Now you can filter and sort
  results_df <- results_df %>%
    filter(!is.na(p_value)) %>%  # Remove entries without valid p-values
    arrange(desc(significant), desc(abs(percent_difference)))  # Sort by significance and absolute difference
  
  # Print any significant findings
  significant_results <- results_df %>% 
    filter(significant == TRUE) 
  
  if (nrow(significant_results) > 0) {
    print("Significant findings:")
    print(head(significant_results, 10))
  } else {
    print("No significant findings detected.")
  }
} else {
  print("No valid results were generated. Check your data structure.")
}
```

Looking at 'Rationale for Match'
```{r}
library(tidyverse)
library(stringr)
library(tidytext)
library(scales)

# First, clean and prepare the text
clean_text <- function(text) {
  if (is.character(text)) {
    # Convert to lowercase
    text <- tolower(text)
    # Remove special characters and extra spaces
    text <- str_replace_all(text, "[^[:alnum:][:space:]]", " ")
    text <- str_squish(text)
    return(text)
  }
  return("")
}

# Apply cleaning to non-null values
rationales <- df %>%
  filter(!is.na(Rationale.for.Match)) %>%
  mutate(clean_rationale = map_chr(Rationale.for.Match, clean_text))

# Create a list of common interest categories to search for
interest_categories <- list(
  sports = c("sport", "baseball", "basketball", "football", "soccer", "tennis", "golf", 
             "swimming", "hockey", "volleyball", "athletic", "exercise", "gym", "workout",
             "running", "biking", "skating", "skiing", "snowboarding", "martial arts"),
  arts = c("art", "drawing", "painting", "music", "singing", "dancing", "crafts", "creative",
           "writing", "poetry", "theatre", "drama", "acting", "photography", "playing music",
           "instrument", "band", "choir", "piano", "guitar"),
  outdoors = c("outdoor", "hiking", "camping", "fishing", "hunting", "nature", "gardening",
               "environmental", "animals", "wildlife", "birds", "park", "beach", "forest", "mountain"),
  academics = c("school", "learning", "study", "education", "academic", "reading", "books",
                "science", "math", "history", "homework", "college", "university", "teaching",
                "student", "classroom", "grades", "tutor"),
  technology = c("technology", "computer", "coding", "programming", "video games", "gaming",
                 "online", "internet", "digital", "software", "hardware", "engineering",
                 "electronics", "robotics", "tech"),
  social = c("social", "talking", "conversation", "friends", "relationships", "community",
             "leadership", "communication", "group", "teamwork", "helping others", "volunteer"),
  food = c("food", "cooking", "baking", "eating", "restaurant", "cuisine", "meal", "recipe",
           "culinary", "chef", "dinner", "lunch", "breakfast", "snack", "dessert"),
  cultural = c("culture", "language", "heritage", "tradition", "religious", "spiritual",
               "ethnic", "diversity", "identity", "cultural activities"),
  personality_traits = c("shy", "outgoing", "quiet", "talkative", "energetic", "calm", "patient",
                         "creative", "organized", "responsible", "humor", "funny", "serious",
                         "thoughtful", "kind", "caring", "empathetic", "confident"),
  family = c("family", "parent", "sibling", "brother", "sister", "mom", "dad", "grandparent",
             "family activities", "family time", "home", "household")
)

# Function to check if any keyword in a category appears in the text
check_category_keywords <- function(text, keywords) {
  if (is.na(text)) return(FALSE)
  
  any(sapply(keywords, function(keyword) {
    # Use word boundaries to match whole words only
    pattern <- paste0("\\b", keyword, "\\b")
    str_detect(text, pattern)
  }))
}

# Create interest category features
print("Creating interest category features...")
for (category_name in names(interest_categories)) {
  column_name <- paste0("interest_", category_name)
  keywords <- interest_categories[[category_name]]
  
  # df[[column_name]] <- FALSE
  
  # df[!is.na(df$Rationale.for.Match), column_name] <- 
  #   sapply(df$Rationale.for.Match[!is.na(df$Rationale.for.Match)], 
  #          function(x) check_category_keywords(clean_text(x), keywords))
  
  # Instead, let's just calculate how many matches we would get, without modifying df
  temp_results <- sapply(df$Rationale.for.Match[!is.na(df$Rationale.for.Match)], 
          function(x) check_category_keywords(clean_text(x), keywords))
  count <- sum(temp_results, na.rm = TRUE)
  percent <- count / nrow(df) * 100
  
  cat(sprintf("- %s: %d matches (%.1f%% of all records)\n", 
              column_name, count, percent))
}

# Extract all individual words for frequency analysis
all_words <- rationales %>%
  unnest_tokens(word, clean_rationale) 

# Count word frequency
word_freq <- all_words %>%
  count(word, sort = TRUE)

# Remove common stop words
stop_words_custom <- c(
  stop_words$word, 
  "share", "shared", "sharing", "interests", "interest", "name", "like", "likes", 
  "including", "include", "well", "just", "about", "think", "want", "wants", 
  "able", "all", "any", "are", "bs", "ls", "bb", "lb", "bbbs", "match", "matches", "matched"
)

filtered_words <- word_freq %>%
  filter(!word %in% stop_words_custom, str_length(word) > 2)

# Get top 50 most common words
top_words <- filtered_words %>%
  slice_head(n = 50)

# Plot the top words
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top 50 Words in Rationale for Match",
       x = "Word",
       y = "Frequency") +
  theme_minimal()

# Create a new feature for each common interest that appears frequently
threshold <- nrow(df) * 0.03  # 3% of records
common_interests <- filtered_words %>%
  filter(n > threshold) %>%
  pull(word)

cat(sprintf("\nFound %d common interests appearing in > 3%% of records:\n", length(common_interests)))
cat(paste(common_interests[1:min(20, length(common_interests))], collapse = ", "))
if (length(common_interests) > 20) cat(", ...")
cat("\n")

# Create binary features for the most common interests
for (interest in common_interests[1:min(20, length(common_interests))]) {
  column_name <- paste0("has_", interest)
  
  # df[[column_name]] <- FALSE
  
  # df[!is.na(df$Rationale.for.Match), column_name] <- 
  #   sapply(df$Rationale.for.Match[!is.na(df$Rationale.for.Match)], 
  #          function(x) str_detect(clean_text(x), paste0("\\b", interest, "\\b")))
  
  # Instead, just calculate counts without modifying df
  temp_results <- sapply(df$Rationale.for.Match[!is.na(df$Rationale.for.Match)], 
           function(x) str_detect(clean_text(x), paste0("\\b", interest, "\\b")))
  count <- sum(temp_results, na.rm = TRUE)
  percent <- count / nrow(df) * 100
  
  cat(sprintf("- %s: %d matches (%.1f%% of records)\n", 
              column_name, count, percent))
}

# Analyze the relationship between interests and match length
# cat("\nAnalyzing relationship between interests and match length...\n")
# interest_columns <- df %>%
#   select(starts_with("interest_") | starts_with("has_")) %>%
#   names()
# 
# interest_impact <- tibble()
# 
# for (col in interest_columns) {
#   # Calculate average match length when interest is present vs. absent
#   avg_length_present <- df %>%
#     filter(.data[[col]] == TRUE) %>%
#     summarize(avg = mean(Match.Length, na.rm = TRUE)) %>%
#     pull(avg)
#   
#   avg_length_absent <- df %>%
#     filter(.data[[col]] == FALSE) %>%
#     summarize(avg = mean(Match.Length, na.rm = TRUE)) %>%
#     pull(avg)
#   
#   diff_percent <- ifelse(avg_length_absent > 0,
#                          ((avg_length_present - avg_length_absent) / avg_length_absent * 100),
#                          Inf)
#   
#   count_present <- sum(df[[col]], na.rm = TRUE)
#   percent_records <- count_present / nrow(df) * 100
#   
#   # Print details
#   cat(sprintf("- %s: Present: %.1f months, Absent: %.1f months (Difference: %+.1f%%)\n",
#              col, avg_length_present, avg_length_absent, diff_percent))
#   
#   # Add to summary dataframe
#   interest_impact <- interest_impact %>%
#     bind_rows(tibble(
#       interest = col,
#       avg_length_present = avg_length_present,
#       avg_length_absent = avg_length_absent,
#       diff_percent = diff_percent,
#       count = count_present,
#       percent_records = percent_records
#     ))
# }
# 
# # Sort by impact on match length
# interest_impact <- interest_impact %>%
#   arrange(desc(diff_percent))
# 
# # Display the top 10 interests that are most strongly associated with longer matches
# cat("\nTop 10 interests associated with LONGER matches:\n")
# interest_impact %>%
#   slice_head(n = 10) %>%
#   select(interest, avg_length_present, avg_length_absent, diff_percent, count, percent_records) %>%
#   print()
# 
# # Display the bottom 10 interests that are most strongly associated with shorter matches
# cat("\nTop 10 interests associated with SHORTER matches:\n")
# interest_impact %>%
#   slice_tail(n = 10) %>%
#   select(interest, avg_length_present, avg_length_absent, diff_percent, count, percent_records) %>%
#   print()
# 
# # Create a visualization of the impact of interests on match length
# interest_impact %>%
#   slice_max(abs(diff_percent), n = 15) %>%
#   mutate(interest = fct_reorder(interest, diff_percent)) %>%
#   ggplot(aes(x = interest, y = diff_percent, fill = diff_percent > 0)) +
#   geom_col() +
#   coord_flip() +
#   scale_fill_manual(values = c("firebrick", "steelblue"),
#                     name = "Associated with",
#                     labels = c("Shorter matches", "Longer matches")) +
#   labs(title = "Impact of Interests on Match Length",
#        subtitle = "Percentage difference in average match length when interest is present vs. absent",
#        x = "Interest Category",
#        y = "% Difference in Match Length") +
#   theme_minimal()
```

Data Cleaning - Changing variable type:
```{r}
# Function to convert data types in a dataframe
convert_df_dtypes <- function(df) {
  # Create a copy to avoid modifying the original dataframe
  df_converted <- df
  
  # Define column groups
  date_columns <- c(
    'Big.Approved.Date', 'Big.Birthdate', 'Match.Activation.Date', 
    'Match.Closure.Meeting.Date', 'Big.Acceptance.Date', 
    'Big.Contact..Created.Date', 'Big.Enrollment..Created.Date',
    'Little.RTBM.Date.in.MF', 'Little.Application.Received',
    'Little.Interview.Date', 'Little.Acceptance.Date', 'Little.Birthdate'
  )
  
  numeric_columns <- c(
    'Big.Age', 'Big.Days.Acceptance.to.Match', 'Big.Days.Interview.to.Acceptance',
    'Big.Days.Interview.to.Match', 'Big.Re.Enroll', 'Little.RTBM.in.Matchforce',
    'Little.Moved.to.RTBM.in.MF', 'Little.Mailing.Address.Census.Block.Group',
    'Big.Home.Census.Block.Group', 'Big.Employer.School.Census.Block.Group',
    'Match.Length'
  )
  
  categorical_candidates <- c(
    'Stage', 'Big.County', 'Big.Occupation', 'Big..Military',
    'Big.Level.of.Education', 'Big.Gender', 'Program', 'Program.Type',
    'Big.Race.Ethnicity', 'Big.Enrollment..Record.Type',
    'Big.Assessment.Uploaded', 'Big.Open.to.Cross.Gender.Match',
    'Big.Contact..Preferred.Communication.Type', 'Big.Contact..Former.Big.Little',
    'Big.Contact..Marital.Status', 'Little.Gender', 'Little.Participant..Race.Ethnicity'
  )
  
  id_columns <- c(
    'Match.ID.18Char', 'Little.ID', 'Big.ID', 'Big.Employer'
  )
  
  # Convert date columns
  for (col in date_columns) {
    if (col %in% names(df_converted)) {
      df_converted <- df_converted %>%
        mutate(!!sym(col) := as.Date(get(col), errors = "coerce"))
    }
  }
  
  # Convert numeric columns
  for (col in numeric_columns) {
    if (col %in% names(df_converted)) {
      if (col == 'Big.Age') {
        # Integer column
        df_converted <- df_converted %>%
          mutate(!!sym(col) := as.integer(get(col)))
      } else {
        # Float columns
        df_converted <- df_converted %>%
          mutate(!!sym(col) := as.numeric(get(col)))
      }
    }
  }
  
  # Convert logical/boolean columns
  logical_cols <- sapply(df_converted, is.logical)
  if (any(logical_cols)) {
    logical_col_names <- names(logical_cols)[logical_cols]
    for (col in logical_col_names) {
      # Keep as logical type
      df_converted <- df_converted %>%
        mutate(!!sym(col) := as.logical(get(col)))
    }
  }
  
  # Convert categorical columns
  for (col in categorical_candidates) {
    if (col %in% names(df_converted)) {
      # Check if has relatively few unique values (less than 5% of row count)
      n_unique <- df_converted %>% 
        select(all_of(col)) %>% 
        n_distinct()
      
      if (n_unique < nrow(df_converted) * 0.05) {
        # Convert to factor (categorical)
        df_converted <- df_converted %>%
          mutate(!!sym(col) := as.factor(get(col)))
      }
    }
  }
  
  # Convert ID columns to character type
  for (col in id_columns) {
    if (col %in% names(df_converted)) {
      df_converted <- df_converted %>%
        mutate(!!sym(col) := as.character(get(col)))
    }
  }
  
  # Convert remaining character/string columns
  for (col in names(df_converted)) {
    if (is.character(df_converted[[col]])) {
      # Already character, no need to convert
      next
    } else if (is.factor(df_converted[[col]])) {
      # Convert factors to character if not in categorical_candidates
      if (!(col %in% categorical_candidates)) {
        df_converted <- df_converted %>%
          mutate(!!sym(col) := as.character(get(col)))
      }
    }
  }
  
  return(df_converted)
}

library(tidyverse)

df_converted <- convert_df_dtypes(df)
df <- df_converted

# Display information about the dataframe
cat("df_converted information:\n")
glimpse(df_converted)

# Display the first few rows
cat("\nFirst few rows:\n")
print(head(df_converted))

# Check for missing values
cat("\nMissing values per column:\n")
missing_values <- sapply(df, function(x) sum(is.na(x)))
print(missing_values)
```
Missingness by Date:
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(lubridate)
library(scales)
library(gridExtra)

# Function to analyze missingness patterns by activation date with quarterly binning
analyze_missingness_by_quarter <- function(df, date_column = 'Match.Activation.Date') {
  
  # Extract year and quarter from activation date and create a new column
  df <- df %>% 
    mutate(Activation_Quarter = paste0(year(get(date_column)), "-Q", quarter(get(date_column))))
  
  # Dictionary to store results
  missingness_over_time <- list()
  
  # Get count of records per quarter for reference
  quarterly_counts <- df %>% 
    group_by(Activation_Quarter) %>% 
    summarize(count = n())
  
  # Calculate missingness for each column by activation quarter
  for (col in colnames(df)) {
    if (col != date_column && col != "Activation_Quarter") {
      # Calculate percent missing for each quarter
      missingness <- df %>% 
        group_by(Activation_Quarter) %>% 
        summarize(missingness = mean(is.na(get(col))) * 100)
      
      missingness_over_time[[col]] <- missingness
    }
  }
  
  return(list(missingness_over_time = missingness_over_time, quarterly_counts = quarterly_counts))
}

# Function to visualize the missingness patterns with quarterly binning
plot_missingness_patterns_by_quarter <- function(df, top_n = 10, date_column = 'Match.Activation.Date') {
  # Calculate overall missingness for each column
  overall_missingness <- df %>%
    summarize(across(everything(), ~mean(is.na(.)) * 100)) %>%
    pivot_longer(cols = everything(), names_to = "column", values_to = "percent_missing") %>%
    arrange(desc(percent_missing))
  
  # Select top N columns with highest missingness
  top_missing_cols <- head(overall_missingness$column, top_n)
  
  # Analyze missingness by quarter
  result <- analyze_missingness_by_quarter(df, date_column)
  missingness_by_quarter <- result$missingness_over_time
  quarterly_counts <- result$quarterly_counts
  
  # Plot 1: Overall missingness bar chart
  p1 <- overall_missingness %>%
    filter(column %in% top_missing_cols) %>%
    ggplot(aes(x = percent_missing, y = reorder(column, percent_missing))) +
    geom_bar(stat = "identity", fill = "skyblue") +
    labs(title = paste("Top", top_n, "Columns with Highest Percentage of Missing Values"),
         x = "Percentage Missing (%)",
         y = "") +
    theme_minimal() +
    scale_x_continuous(limits = c(0, 100))
  
  # Plot 2: Missingness over time for top 5-10 columns (or choose subset if fewer than 10)
  top5_missing_cols <- if(length(top_missing_cols) > 5) top_missing_cols[6:min(10, length(top_missing_cols))] else top_missing_cols
  
  # Create a data frame for plotting
  plot_data <- data.frame()
  
  for (col in top5_missing_cols) {
    if (col %in% names(missingness_by_quarter)) {
      temp_data <- missingness_by_quarter[[col]]
      temp_data$column <- col
      plot_data <- rbind(plot_data, temp_data)
    }
  }
  
  # Only proceed with this plot if we have data
  if(nrow(plot_data) > 0) {
    # Sort periods for chronological display
    all_quarters <- unique(plot_data$Activation_Quarter)
    # Create a sorting key for proper chronological ordering
    quarter_order <- order(sapply(strsplit(all_quarters, "-Q"), function(x) as.numeric(paste0(x[1], ".0", x[2]))))
    all_quarters <- all_quarters[quarter_order]
    
    # Convert to factor to maintain order
    plot_data$Activation_Quarter <- factor(plot_data$Activation_Quarter, levels = all_quarters)
    
    p2 <- ggplot(plot_data, aes(x = Activation_Quarter, y = missingness, group = column, color = column)) +
      geom_line() +
      geom_point() +
      labs(title = "Missing Data Percentage by Quarter for Selected Columns",
           x = "Activation Quarter",
           y = "Percentage Missing (%)") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            legend.title = element_blank()) +
      scale_y_continuous(limits = c(0, 100)) +
      scale_color_viridis_d()
    
    # Plot 3: Record count by quarter
    quarterly_counts$Activation_Quarter <- factor(quarterly_counts$Activation_Quarter, levels = all_quarters)
    
    p3 <- ggplot(quarterly_counts, aes(x = Activation_Quarter, y = count)) +
      geom_bar(stat = "identity", fill = "lightblue", alpha = 0.7) +
      labs(title = "Number of Records by Quarter",
           x = "Activation Quarter",
           y = "Count") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Arrange plots
    grid.arrange(p1, p2, p3, nrow = 3, heights = c(3, 4, 1))
  } else {
    # If we don't have data for the second plot, just show the first and third
    p3 <- ggplot(quarterly_counts, aes(x = Activation_Quarter, y = count)) +
      geom_bar(stat = "identity", fill = "lightblue", alpha = 0.7) +
      labs(title = "Number of Records by Quarter",
           x = "Activation Quarter",
           y = "Count") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    grid.arrange(p1, p3, nrow = 2, heights = c(3, 1))
  }
}

df$Match.Activation.Date <- as.Date(df$Match.Activation.Date) 
plot_missingness_patterns_by_quarter(df)
```


removing high missingness data
```{r}

# Calculate the number of missing values for each column
na_counts <- sapply(df, function(x) sum(is.na(x)))

# Print columns sorted by number of missing values (optional)
na_counts_sorted <- sort(na_counts, decreasing = TRUE)
print(na_counts_sorted)

# Identify columns with more than 3000 missing values
high_missing_cols <- names(na_counts[na_counts > 1900])
print(paste("Columns with >3000 missing values to remove:", length(high_missing_cols)))
print(high_missing_cols)

# Check if we'd be removing all columns
if (length(high_missing_cols) == ncol(df)) {
  print("WARNING: All columns have >3000 missing values. No columns will be removed.")
} else if (length(high_missing_cols) > 0) {
  # Create new dataframe without high-missingness columns
  df_reduced <- df[, !names(df) %in% high_missing_cols]
  print(paste("Original dimensions:", dim(df)[1], "rows,", dim(df)[2], "columns"))
  print(paste("New dimensions:", dim(df_reduced)[1], "rows,", dim(df_reduced)[2], "columns"))
  
  # Assign to df only if the result is not empty
  if (ncol(df_reduced) > 0) {
    df <- df_reduced
    print("Successfully removed high-missingness columns.")
  } else {
    print("ERROR: Removing these columns would result in an empty dataframe. No changes made.")
  }
} else {
  print("No columns with >3000 missing values found.")
}

```


Looking at `Big.Days.Acceptance.To.Match`
```{r}
sum(is.na(df$`Big.Days.Acceptance.to.Match`))
```


```{r}
result_big_days <- compare_match_length_by_missingness(df, "Big.Days.Acceptance.to.Match")
print(result_big_days)
```
```{r}
# Filter the missingness analysis only for 'Big.Days.Acceptance.to.Match'
result <- analyze_missingness_by_quarter(df, date_column = 'Match.Activation.Date')
big_days_missingness <- result$missingness_over_time[["Big.Days.Acceptance.to.Match"]]
quarterly_counts <- result$quarterly_counts

# Ensure data is available
if (!is.null(big_days_missingness) && nrow(big_days_missingness) > 0) {
  # Sort quarters for chronological order
  all_quarters <- unique(big_days_missingness$Activation_Quarter)
  quarter_order <- order(sapply(strsplit(all_quarters, "-Q"), function(x) as.numeric(paste0(x[1], ".0", x[2]))))
  all_quarters <- all_quarters[quarter_order]
  
  # Convert to factor to maintain order
  big_days_missingness$Activation_Quarter <- factor(big_days_missingness$Activation_Quarter, levels = all_quarters)
  quarterly_counts$Activation_Quarter <- factor(quarterly_counts$Activation_Quarter, levels = all_quarters)
  
  # Plot missingness over time
  p1 <- ggplot(big_days_missingness, aes(x = Activation_Quarter, y = missingness)) +
    geom_line(color = "red") +
    geom_point(color = "red") +
    labs(title = "Missing Data Percentage for Big.Days.Acceptance.to.Match by Quarter",
         x = "Activation Quarter",
         y = "Percentage Missing (%)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, 100))
  
  # Plot record count per quarter
  p2 <- ggplot(quarterly_counts, aes(x = Activation_Quarter, y = count)) +
    geom_bar(stat = "identity", fill = "lightblue", alpha = 0.7) +
    labs(title = "Number of Records by Quarter",
         x = "Activation Quarter",
         y = "Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  # Arrange plots
  grid.arrange(p1, p2, nrow = 2, heights = c(3, 1))
} else {
  print("No missingness data available for Big.Days.Acceptance.to.Match")
}

```
Heavily correlated with date - maybe date is a big feature in predicting Match Length?


```{r}
library(ggplot2)
library(dplyr)

#df$Match.Activation.Date <- as.Date("1940-01-01") + as.numeric(df$Match.Activation.Date)

# Ensure the date column is in Date format
df <- df %>%
  mutate(Match.Activation.Date = as.Date(Match.Activation.Date))

# Create the plot
ggplot(df, aes(x = Match.Activation.Date, y = Match.Length)) +
  geom_point(alpha = 0.5, color = "blue") +  # Scatter plot with transparency
  geom_smooth(method = "loess", color = "red", se = TRUE) +  # Trend line
  labs(title = "Match Length Over Activation Date",
       x = "Activation Date",
       y = "Match Length") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
explore scaling of match length - proportion of time possible? normalization based on how long ago- maybe get current matches? or just current matches from recent - SURVIVAL ANALYSIS! - Log rank test - for differences in groups when it comes to target variable. Probably heavily right tailed data - consider logging - Kaplan Meier Curve - Proportional Hazards model 
Impute data first, then try proportional hazards model.

look at imputation on date? amelia
Trying Amelia: - didnt work

```{r}
df
```

'Closure.Reason', 'Big.Occupation', 'Big.Languages' - Text fields - synthesize into smaller categories
```{r}
# Function to categorize text fields based on keywords
categorize_text <- function(text_vector, category_rules, default_category = "Other") {
  result <- rep(default_category, length(text_vector))
  
  if(any(is.na(text_vector))) {
    result[is.na(text_vector)] <- NA
  }
  
  for(i in 1:length(category_rules)) {
    category_name <- names(category_rules)[i]
    keywords <- category_rules[[i]]
    
    for(j in which(!is.na(text_vector))) {
      if(any(sapply(keywords, function(kw) grepl(kw, text_vector[j], ignore.case = TRUE)))) {
        result[j] <- category_name
      }
    }
  }
  
  result <- factor(result)
  return(result)
}

# Define category rules for each text field
closure_reason_rules <- list(
  "Scheduling_Issues" = c("schedule", "time", "availability", "busy"),
  "Relationship_Problems" = c("relationship", "conflict", "disagree", "personal", "not compatible"),
  "Relocation" = c("move", "moved", "relocation", "relocate", "different city", "different state"),
  "Family_Issues" = c("family", "parent", "guardian", "parental"),
  "School_Issues" = c("school", "academic", "education", "grade"),
  "Health_Issues" = c("health", "illness", "medical", "sick", "disease"),
  "Behavior_Issues" = c("behavior", "conduct", "attitude", "disciplin"),
  "Program_Requirements" = c("requirement", "qualify", "eligibility", "criteria", "guideline")
)

occupation_rules <- list(
  "Business_Finance" = c("account", "financ", "budget", "analyst", "bank", "economic", "market", "business"),
  "Education" = c("teach", "professor", "instructor", "education", "academic", "school", "college", "university"),
  "Healthcare" = c("doctor", "nurse", "medical", "health", "dental", "therapist", "clinic", "hospital"),
  "Technology" = c("software", "developer", "engineer", "IT", "computer", "tech", "program", "web", "data"),
  "Legal" = c("lawyer", "attorney", "legal", "law", "judge", "paralegal"),
  "Arts_Media" = c("artist", "design", "writer", "media", "journalist", "creative", "music", "film"),
  "Service_Industry" = c("retail", "sales", "service", "hospitality", "restaurant", "customer"),
  "Trades_Labor" = c("construct", "mechanic", "carpenter", "electric", "plumb", "repair", "builder", "labor"),
  "Student" = c("student", "graduate", "undergrad"),
  "Retired" = c("retire")
)

languages_rules <- list(
  "English_Only" = c("english only", "only english", "just english"),
  "Spanish" = c("spanish", "español"),
  "French" = c("french", "français"),
  "Asian_Languages" = c("chinese", "mandarin", "cantonese", "japanese", "korean", "vietnamese", "thai"),
  "European_Languages" = c("german", "italian", "portuguese", "dutch", "russian", "polish", "greek"),
  "Middle_Eastern" = c("arabic", "hebrew", "farsi", "turkish", "persian"),
  "Multiple_Languages" = c("bilingual", "multilingual", "fluent in", "speaks multiple")
)

# Example usage:
# Assuming your data is in a dataframe called 'data'
df$Closure_Reason_Category <- categorize_text(df$Closure.Reason, closure_reason_rules)
df$Occupation_Category <- categorize_text(df$Big.Occupation, occupation_rules)
df$Languages_Category <- categorize_text(df$Big.Languages, languages_rules)
```

Looking at Closure_Reason:

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(forcats)

# Assuming 'df' is your dataframe with the data
# Let's first look at summary statistics for Match.Length by Closure.Reason
match_summary <- df %>%
  group_by(Closure_Reason_Category) %>%
  summarise(
    count = n(),
    mean_length = mean(Match.Length, na.rm = TRUE),
    median_length = median(Match.Length, na.rm = TRUE),
    sd_length = sd(Match.Length, na.rm = TRUE),
    min_length = min(Match.Length, na.rm = TRUE),
    max_length = max(Match.Length, na.rm = TRUE)
  ) %>%
  arrange(desc(count))

# Print summary
print(match_summary)

# 1. Boxplot of Match.Length by Closure_Reason_Category
closure_boxplot <- ggplot(df, aes(x = reorder(Closure_Reason_Category, Match.Length, FUN = median), 
                                  y = Match.Length, 
                                  fill = Closure_Reason_Category)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Distribution of Match Length by Closure Reason",
       x = "Closure Reason",
       y = "Match Length") +
  theme_minimal() +
  theme(legend.position = "none")

print(closure_boxplot)

# 2. Barplot for average Match.Length by Closure_Reason_Category
avg_match_length <- df %>%
  group_by(Closure_Reason_Category) %>%
  summarise(avg_length = mean(Match.Length, na.rm = TRUE),
            count = n()) %>%
  arrange(desc(avg_length))

avg_length_plot <- ggplot(avg_match_length, aes(x = reorder(Closure_Reason_Category, avg_length), 
                                               y = avg_length, 
                                               fill = count)) +
  geom_col() +
  coord_flip() +
  scale_fill_viridis_c(name = "Count") +
  labs(title = "Average Match Length by Closure Reason",
       x = "Closure Reason",
       y = "Average Match Length") +
  theme_minimal()

print(avg_length_plot)

# 3. Density plot for Match.Length by top closure reasons
# Select top 5 most common closure reasons for clarity
top_reasons <- match_summary %>%
  top_n(5, count) %>%
  pull(Closure_Reason_Category)

density_plot <- df %>%
  filter(Closure_Reason_Category %in% top_reasons) %>%
  ggplot(aes(x = Match.Length, fill = Closure_Reason_Category)) +
  geom_density(alpha = 0.7) +
  labs(title = "Density Distribution of Match Length by Top 5 Closure Reasons",
       x = "Match Length",
       y = "Density") +
  theme_minimal()

print(density_plot)

# 4. Histogram of Match.Length faceted by Closure_Reason_Category
histogram_plot <- df %>%
  filter(Closure_Reason_Category %in% top_reasons) %>%
  ggplot(aes(x = Match.Length, fill = Closure_Reason_Category)) +
  geom_histogram(binwidth = 30, alpha = 0.8) +
  facet_wrap(~Closure_Reason_Category, scales = "free_y") +
  labs(title = "Distribution of Match Length by Top Closure Reasons",
       x = "Match Length (days)",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

print(histogram_plot)

# 5. Create a violin plot to show distribution shapes
violin_plot <- df %>%
  filter(Closure_Reason_Category %in% top_reasons) %>%
  ggplot(aes(x = Closure_Reason_Category, y = Match.Length, fill = Closure_Reason_Category)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", alpha = 0.5) +
  coord_flip() +
  labs(title = "Violin Plot of Match Length by Top Closure Reasons",
       x = "Closure Reason",
       y = "Match Length") +
  theme_minimal() +
  theme(legend.position = "none")

print(violin_plot)

# 6. Scatter plot with jittering to show individual data points
jitter_plot <- df %>%
  filter(Closure_Reason_Category %in% top_reasons) %>%
  ggplot(aes(x = Closure_Reason_Category, y = Match.Length, color = Closure_Reason_Category)) +
  geom_jitter(alpha = 0.5, width = 0.3) +
  stat_summary(fun = mean, geom = "point", shape = 18, size = 4, color = "black") +
  coord_flip() +
  labs(title = "Individual Match Lengths by Closure Reason (with means)",
       x = "Closure Reason",
       y = "Match Length") +
  theme_minimal()

print(jitter_plot)

# 7. Cumulative distribution function plot
cdf_plot <- df %>%
  filter(Closure_Reason_Category %in% top_reasons) %>%
  ggplot(aes(x = Match.Length, color = Closure_Reason_Category)) +
  stat_ecdf() +
  labs(title = "Cumulative Distribution of Match Length by Closure Reason",
       x = "Match Length",
       y = "Cumulative Proportion") +
  theme_minimal()

print(cdf_plot)

# 8. Proportion of closure reasons
prop_plot <- df %>%
  count(Closure_Reason_Category) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = "", y = prop, fill = Closure_Reason_Category)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(title = "Proportion of Closure Reasons") +
  theme_void()

print(prop_plot)
```
```{r}
df$Closure_Reason_Category <- NULL
```

```{r}
# Closure.Reason basically covers Closure.Details - remove it
df$Closure.Details <- NULL
df$Closure.Reason <- NULL
df$Rationale.for.Match <- NULL
df$Big.Occupation <- NULL
df$Big.Languages <- NULL
df$Big.Home.Census.Block.Group <- NULL
df$Little.Home.Census.Block.Group <- NULL
df$Program <- NULL
df$Program.Type <- NULL
df
```

Add race alignments
```{r}
# Function to check if Big and Little ethnicities share any keywords
check_ethnicity_match <- function(df) {
  # Create a new column to store the matching result
  df$Ethnicity_Match <- FALSE
  
  # Loop through each row
  for (i in 1:nrow(df)) {
    # Get the Big and Little race/ethnicity values
    big_race <- df$Big.Race.Ethnicity[i]
    little_race <- df$Little.Participant..Race.Ethnicity[i]
    
    # Skip if either value is NA
    if (is.na(big_race) || is.na(little_race)) {
      df$Ethnicity_Match[i] <- NA
      next
    }
    
    # Convert to character (in case they're factors)
    big_race <- as.character(big_race)
    little_race <- as.character(little_race)
    
    # Split strings by semicolons to handle multiple ethnicities
    big_races <- unlist(strsplit(big_race, ";"))
    little_races <- unlist(strsplit(little_race, ";"))
    
    # Clean up any leading/trailing spaces
    big_races <- trimws(big_races)
    little_races <- trimws(little_races)
    
    # Check if there's any match
    match_found <- FALSE
    for (b in big_races) {
      for (l in little_races) {
        # Extract keywords to compare (simplify the comparison)
        keywords <- c("White", "Black", "Asian", "Hispanic", "Indian", "Alaska", 
                     "Middle Eastern", "North African", "Other")
        
        # Check for each keyword
        for (keyword in keywords) {
          if (grepl(keyword, b, ignore.case = TRUE) && 
              grepl(keyword, l, ignore.case = TRUE)) {
            match_found <- TRUE
            break
          }
        }
        if (match_found) break
      }
      if (match_found) break
    }
    
    # Assign the result
    df$Ethnicity_Match[i] <- match_found
  }
  
  return(df)
}

df <- check_ethnicity_match(df)
df$Big.Race.Ethnicity <- NULL
df$Little.Participant..Race.Ethnicity <- NULL
```



```{r}
df$Big.Employer <- NULL
```


Col list:
Match.ID.18Char, Stage, Little.ID, Big.ID, Big.County, Big.Age, Big.Approved.Date, Big.Gender, Big.Birthdate, Program, Program.Type, Match.Activation.Date, Little.Gender, Little.Birthdate, Little.Mailing.Address.Census.Block.Group, Big.Home.Census.Block.Group, Match.Length, interest_sports, interest_arts, interest_outdoors, interest_academics, interest_technology, interest_social, interest_food, interest_cultural, interest_personality_traits, interest_family, has_distance, has_traits, has_active, has_miles, has_sports, has_friendly, has_talkative, has_activities, has_outgoing, has_arts, has_outdoors, has_crafts, has_games, has_enjoy, has_min, has_mins, has_curious, has_parks, has_movies, has_personality, Occupation_Category, Languages_Category, Employer.Category, Ethnicity_Match

```{r}
# Load necessary libraries
library(tidyverse)
library(lubridate)
library(mice)

# Define column types as specified
id_columns <- c("Match.ID.18Char", "Little.ID", "Big.ID")

date_columns <- c(
  'Big.Approved.Date', 'Big.Birthdate', 'Match.Activation.Date', 'Little.Birthdate'
)

numeric_columns <- c(
  'Big.Age', 'Match.Length', 
  'Little.Mailing.Address.Census.Block.Group', 'Big.Home.Census.Block.Group'
)

categorical_columns <- c(
  'Stage', 'Big.Gender', 'Big.County', 'Little.Gender', 'Occupation_Category', 
  'Languages_Category', 'Employer.Category', 'Ethnicity_Match'
)

# Verify all columns are accounted for
all_defined_columns <- c(id_columns, date_columns, numeric_columns, categorical_columns)
unaccounted_columns <- setdiff(names(df), all_defined_columns)

if (length(unaccounted_columns) > 0) {
  cat("Warning: The following columns are not categorized:", 
      paste(unaccounted_columns, collapse=", "), "\n")
}


# 1. Simple Imputation Methods
df_simple <- df

# For date columns - impute with median date
for (col in date_columns) {
  if (col %in% names(df) && sum(is.na(df[[col]])) > 0) {
    # Calculate median date excluding NAs
    median_date <- median(df[[col]], na.rm = TRUE)
    
    # If all values are NA, set a default date
    if (is.na(median_date)) {
      median_date <- as.Date("2017-01-01")  # A reasonable default based on your data
    }
    
    # Impute NAs with median
    df_simple[[col]][is.na(df_simple[[col]])] <- median_date
  }
}

# For numeric columns - impute with mean or median
for (col in numeric_columns) {
  if (col %in% names(df) && sum(is.na(df[[col]])) > 0) {
    # Special handling for census block groups (use mode instead of mean)
    if (grepl("Census.Block.Group", col)) {
      # For census block groups, find the most common value
      mode_value <- as.numeric(names(sort(table(df[[col]]), decreasing = TRUE)[1]))
      df_simple[[col]][is.na(df_simple[[col]])] <- mode_value
    } else {
      # For other numeric values, use the mean
      mean_value <- mean(df[[col]], na.rm = TRUE)
      df_simple[[col]][is.na(df_simple[[col]])] <- mean_value
    }
  }
}

# For categorical columns - impute with mode (most frequent value) - COME BACK TO THIS LATER WHEN AMANDA RESPONDS
for (col in categorical_columns) {
  if (col %in% names(df) && sum(is.na(df[[col]])) > 0) {
    # Special handling for Closure_Reason_Category - only impute for closed matches
    if (col == "Closure_Reason_Category") {
      # For active matches, keep NA
      closed_indices <- which(df$Stage == "Closed" & is.na(df[[col]]))
      
      if (length(closed_indices) > 0) {
        # For closed matches missing reason, use most common reason
        mode_value <- names(sort(table(df[[col]]), decreasing = TRUE)[1])
        df_simple[[col]][closed_indices] <- mode_value
      }
    } else {
      # For other categoricals, use the mode
      mode_value <- names(sort(table(df[[col]]), decreasing = TRUE)[1])
      df_simple[[col]][is.na(df_simple[[col]])] <- mode_value
    }
  }
}


# ========= EVALUATE IMPUTATION METHODS =========

# Function to compare original values with imputed values for non-missing data
# (This is a simulation to estimate imputation accuracy)
evaluate_imputation <- function(original_df, imputed_df, method_name) {
  cat("\nEvaluating", method_name, "imputation accuracy:\n")
  
  # For numeric columns
  numeric_eval <- list()
  for (col in intersect(numeric_columns, names(original_df))) {
    if (sum(!is.na(original_df[[col]])) > 10) {  # Only evaluate columns with enough data
      # Randomly select 10% of non-missing values
      non_missing_indices <- which(!is.na(original_df[[col]]))
      if (length(non_missing_indices) > 5) {
        test_indices <- sample(non_missing_indices, ceiling(length(non_missing_indices) * 0.1))
        
        # Save original values
        test_values <- original_df[[col]][test_indices]
        
        # Create a copy with these values set to NA
        test_df <- original_df
        test_df[[col]][test_indices] <- NA
        
        # Apply the imputation method (simplified for this example)
        if (method_name == "Simple") {
          imputed_values <- rep(mean(test_df[[col]], na.rm = TRUE), length(test_indices))
        } else {
          imputed_values <- imputed_df[[col]][test_indices]
        }
        
        # Calculate error metrics
        mae <- mean(abs(test_values - imputed_values), na.rm = TRUE)
        rmse <- sqrt(mean((test_values - imputed_values)^2, na.rm = TRUE))
        
        numeric_eval[[col]] <- c(MAE = mae, RMSE = rmse)
      }
    }
  }
  
  # Print numeric evaluation results
  if (length(numeric_eval) > 0) {
    cat("Numeric column imputation errors:\n")
    numeric_results <- do.call(rbind, numeric_eval)
    print(numeric_results)
  }
  
  # For categorical columns (calculate accuracy)
  cat_eval <- list()
  for (col in intersect(categorical_columns, names(original_df))) {
    if (sum(!is.na(original_df[[col]])) > 10) {  # Only evaluate columns with enough data
      # Randomly select 10% of non-missing values
      non_missing_indices <- which(!is.na(original_df[[col]]))
      if (length(non_missing_indices) > 5) {
        test_indices <- sample(non_missing_indices, ceiling(length(non_missing_indices) * 0.1))
        
        # Save original values
        test_values <- original_df[[col]][test_indices]
        
        # Apply the imputation method (simplified)
        imputed_values <- imputed_df[[col]][test_indices]
        
        # Calculate accuracy
        accuracy <- mean(test_values == imputed_values, na.rm = TRUE)
        cat_eval[[col]] <- accuracy
      }
    }
  }
  
  # Print categorical evaluation results
  if (length(cat_eval) > 0) {
    cat("\nCategorical column imputation accuracy:\n")
    cat_results <- do.call(rbind, as.data.frame(cat_eval))
    colnames(cat_results) <- "Accuracy"
    print(cat_results)
  }
}

# Evaluate each imputation method
evaluate_imputation(df, df_simple, "Simple")


# Final summary
cat("\n==== IMPUTATION SUMMARY ====\n")
cat("Original data had", sum(colSums(is.na(df)) > 0), "columns with missing values\n")
cat("Simple imputation left", sum(colSums(is.na(df_simple)) > 0), "columns with missing values\n")


df
df_simple
```

TRY PROPORTIONAL HAZARDS MODEL HERE:
Check for multicollinearity: # TO BE CONTINUED'

```{r}
library(gmodels)

# Function to perform Fisher's exact tests and filter results by significance
test_categorical_associations <- function(df, categorical_columns, p_threshold = 0.01) {
  # Create empty vectors to store results
  var1_vec <- c()
  var2_vec <- c()
  p_values <- c()
  
  # For each categorical predictor
  for(cat_var in categorical_columns) {
    if(cat_var %in% names(df)) {
      # Check its relationship with all other categorical variables
      for(other_var in categorical_columns) {
        if(other_var != cat_var && other_var %in% names(df)) {
          # Skip if either variable has only one level
          if(length(unique(df[[cat_var]])) <= 1 || length(unique(df[[other_var]])) <= 1) {
            next
          }
          
          # Perform Fisher's exact test
          tryCatch({
            test_result <- fisher.test(df[[cat_var]], df[[other_var]], 
                                      simulate.p.value = TRUE)
            
            # Store results
            var1_vec <- c(var1_vec, cat_var)
            var2_vec <- c(var2_vec, other_var)
            p_values <- c(p_values, test_result$p.value)
          }, error = function(e) {
            message("Error testing ", cat_var, " with ", other_var, ": ", e$message)
          })
        }
      }
    }
  }
  
  # Create data frame of results
  results <- data.frame(
    var1 = var1_vec,
    var2 = var2_vec,
    p_value = p_values
  )
  
  # Filter significant associations
  significant <- results[results$p_value < p_threshold, ]
  
  # Sort by p-value
  significant <- significant[order(significant$p_value), ]
  
  # Return both the full results and filtered results
  return(list(
    all_results = results,
    significant = significant
  ))
}

results <- test_categorical_associations(df_simple, categorical_columns, 0.01)
print(results$significant)
```
might just have to restart :(
```{r}
df_simple
```

all the has / interest vars are highly multicollinear - remove for now?

```{r}
# Load required packages
library(car)

# Creating a formula for the model
# We need a dependent variable - for VIF calculation, it doesn't matter which one we use
# We'll use the first categorical column as the dependent variable for the formula
dependent_var <- 'Match.Length'
independent_vars <- c(categorical_columns[-1], numeric_columns)

# Filter to only include columns that exist in the dataframe
independent_vars <- independent_vars[independent_vars %in% names(df_simple)]

# Create formula string
formula_str <- paste(dependent_var, "~", paste(independent_vars, collapse = " + "))
formula_obj <- as.formula(formula_str)

# Create the model
model <- try(lm(formula_obj, data = df_simple, na.action = na.omit))

# Check for multicollinearity using VIF (Variance Inflation Factor)
if (!inherits(model, "try-error")) {
  # Calculate VIF
  vif_result <- try(vif(model))
  
  if (!inherits(vif_result, "try-error")) {
    print("VIF Results (values > 5 indicate potential multicollinearity, > 10 indicate severe multicollinearity):")
    print(vif_result)
    
    # Identify high VIF values
    high_vif <- vif_result[vif_result > 5]
    if (length(high_vif) > 0) {
      print("Variables with high multicollinearity (VIF > 5):")
      print(high_vif)
    } else {
      print("No severe multicollinearity detected (all VIF values < 5).")
    }
  } else {
    # If VIF calculation fails, try a correlation matrix approach
    print("VIF calculation failed. Using correlation matrix instead.")
    
    # Select only numeric columns for correlation matrix
    numeric_data <- df_simple[, sapply(df_simple, is.numeric)]
    
    if (ncol(numeric_data) > 1) {
      cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")
      print("Correlation Matrix:")
      print(cor_matrix)
      
      # Find high correlations
      high_cors <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)
      if (nrow(high_cors) > 0) {
        print("Variable pairs with high correlation (|r| > 0.7):")
        for (i in 1:nrow(high_cors)) {
          if (high_cors[i, 1] < high_cors[i, 2]) {  # Avoid printing both (A,B) and (B,A)
            var1 <- rownames(cor_matrix)[high_cors[i, 1]]
            var2 <- colnames(cor_matrix)[high_cors[i, 2]]
            cor_val <- cor_matrix[high_cors[i, 1], high_cors[i, 2]]
            print(paste(var1, "and", var2, ":", round(cor_val, 3)))
          }
        }
      } else {
        print("No high correlations (|r| > 0.7) found between numeric variables.")
      }
    } else {
      print("Not enough numeric columns for correlation analysis.")
    }
  }
} else {
  print("Model creation failed. This could be due to perfect multicollinearity or other issues.")
  
  # Try pairwise correlations instead
  print("Using correlation matrix for numeric variables instead.")
  numeric_data <- df_simple[, sapply(df_simple, is.numeric)]
  
  if (ncol(numeric_data) > 1) {
    cor_matrix <- cor(numeric_data, use = "pairwise.complete.obs")
    print("Correlation Matrix:")
    print(cor_matrix)
    
    # Find high correlations
    high_cors <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)
    if (nrow(high_cors) > 0) {
      print("Variable pairs with high correlation (|r| > 0.7):")
      for (i in 1:nrow(high_cors)) {
        if (high_cors[i, 1] < high_cors[i, 2]) {  # Avoid printing both (A,B) and (B,A)
          var1 <- rownames(cor_matrix)[high_cors[i, 1]]
          var2 <- colnames(cor_matrix)[high_cors[i, 2]]
          cor_val <- cor_matrix[high_cors[i, 1], high_cors[i, 2]]
          print(paste(var1, "and", var2, ":", round(cor_val, 3)))
        }
      }
    } else {
      print("No high correlations (|r| > 0.7) found between numeric variables.")
    }
  } else {
    print("Not enough numeric columns for correlation analysis.")
  }
}
```

```{r}
df_simple
```


```{r}
library(survival)
library(dplyr)
library(ggplot2)
library(survminer)
library(Hmisc)
library(pec) 
library(timeROC) 

df_model <- df_simple %>%
  mutate(across(all_of(date_columns), ~as.Date(.)))

# 2. Calculate survival time and event indicator
# Create time variable (in days) and event indicator (1=closed, 0=ongoing/censored)
df_model <- df_model %>%
  mutate(
    # Calculate match duration (time variable)
    match_duration = as.numeric(difftime(
      # If match is closed (Stage=="Closed"), use current date as end date for calculation
      # Otherwise (Stage=="Open"), also use current date (these will be censored observations)
      Sys.Date(),
      Match.Activation.Date, 
      units = "days"
    )),
    event = ifelse(Stage == "Closed", 1, 0)
  )

# 3. Build the Cox Proportional Hazards model
# Start with a simpler model with key variables
cox_model <- coxph(
  Surv(match_duration, event) ~ 
    Big.Age + 
    Big.Gender +
    Occupation_Category +
    Languages_Category +
    Ethnicity_Match,
  data = df_model
)

# Display model summary
summary(cox_model)

# 4. Check proportional hazards assumption
ph_test <- cox.zph(cox_model)
print(ph_test)
plot(ph_test)

# 5. Create Kaplan-Meier survival curves for a key categorical variable (e.g., Program.Type)
km_fit <- survfit(Surv(match_duration, event) ~ Program.Type, data = df_model)
print(km_fit)

# Plot survival curves
ggsurvplot(
  km_fit,
  data = df_model,
  risk.table = TRUE,
  pval = TRUE,
  conf.int = TRUE,
  xlim = c(0, 1000),  # Adjust as needed
  xlab = "Days",
  ylab = "Match Survival Probability",
  title = "Match Survival by Program Type",
  ggtheme = theme_minimal()
)

# 6. Visualize hazard ratios
ggforest(
  cox_model,
  data = df_model,
  main = "Hazard Ratios from Cox Proportional Hazards Model"
)

# 7. For a comprehensive model with all variables
# Note: This might be too complex and lead to overfitting
# Consider variable selection methods if needed

# Example for a more comprehensive model
# comprehensive_model <- coxph(
#   Surv(match_duration, event) ~ .,
#   data = df_model %>% select(match_duration, event, all_of(numeric_columns), all_of(categorical_columns))
# )

# 9. Model validation
# Split data into training and testing sets for validation
set.seed(8080)
train_indices <- sample(1:nrow(df_model), 0.8 * nrow(df_model))
train_data <- df_model[train_indices, ]
test_data <- df_model[-train_indices, ]

# Train model on training data
train_model <- coxph(
  Surv(match_duration, event) ~ 
    Big.Age + 
    Big.Gender + 
    Occupation_Category +
    Languages_Category +
    Ethnicity_Match,
  data = train_data,
  x = TRUE  # This is the key addition
)

# Predict on test data
pred <- predict(train_model, newdata = test_data, type = "risk")

# Calculate concordance index using Hmisc package
concordance <- rcorr.cens(-pred, Surv(test_data$match_duration, test_data$event))
print(paste("Concordance Index:", concordance[1]))
```

```{r}
# 6. Check proportional hazards assumption
ph_test <- cox.zph(train_model)
print(ph_test)
plot(ph_test)

# 7. Validation metrics

# Concordance index (C-index)
pred_risk <- predict(train_model, newdata = test_data, type = "risk")
c_index <- survConcordance(Surv(test_data$match_duration, test_data$event) ~ pred_risk)
print(paste("Concordance Index (C-index):", round(c_index$concordance, 3)))

# 8. Time-dependent ROC curves and AUC at specific time points
# Choose time points to evaluate prediction accuracy
times <- c(365, 730, 1095)  # 1, 2, and 3 years

# Calculate linear predictor for test data
lp <- predict(train_model, newdata = test_data, type = "lp")

# Create time-dependent ROC curves
troc <- timeROC(
  T = test_data$match_duration,
  delta = test_data$event,
  marker = lp,
  cause = 1,
  times = times,
  ROC = TRUE
)

# Print AUC at each time point
print("Time-dependent AUC values:")
for (i in 1:length(times)) {
  print(paste("AUC at", times[i]/365, "years:", round(troc$AUC[i], 3)))
}

# Plot time-dependent ROC curves
plot(troc, time = times[1], title = paste("ROC curve at", times[1]/365, "year"))
plot(troc, time = times[2], add = TRUE, col = "red")
plot(troc, time = times[3], add = TRUE, col = "blue")
legend("bottomright", 
       legend = c(paste(times[1]/365, "year"), paste(times[2]/365, "years"), paste(times[3]/365, "years")),
       col = c("black", "red", "blue"), 
       lwd = 2)

# 9. Brier Score (prediction error)
pec_data <- pec::pec(
  list(Cox = train_model),
  data = test_data,
  formula = Surv(match_duration, event) ~ 1,
  times = times,
  exact = FALSE,
  cens.model = "marginal"
)

# Plot prediction error curves
plot(pec_data, legend = TRUE)

# Print Brier scores at each time point
print("Brier Scores at different time points (lower is better):")
brier_scores <- pec::crps(pec_data)
print(brier_scores)

# 10. Calibration plot
# Calculate predicted survival probabilities at 1 year
time_point <- 365  # 1 year
surv_prob <- predictSurvProb(train_model, newdata = test_data, times = time_point)

# Create calibration plot
test_data$pred_surv <- surv_prob
test_data$pred_group <- cut(test_data$pred_surv, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)

# Calculate actual survival for each group
calibration <- test_data %>%
  group_by(pred_group) %>%
  summarize(
    n = n(),
    pred_surv_mean = mean(pred_surv),
    actual_surv = sum(match_duration > time_point & event == 0) / n(),
    .groups = "drop"
  )

# Print calibration table
print("Calibration table for 1-year survival:")
print(calibration)

# Plot calibration
ggplot(calibration, aes(x = pred_surv_mean, y = actual_surv)) +
  geom_point(aes(size = n)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  xlim(0, 1) + ylim(0, 1) +
  labs(
    x = "Predicted 1-year survival probability",
    y = "Observed 1-year survival probability",
    title = "Calibration plot for 1-year survival predictions"
  ) +
  theme_minimal()

# 11. Identifying influential variables
coef_df <- as.data.frame(summary(train_model)$coefficients)
coef_df$variable <- rownames(coef_df)
coef_df <- coef_df %>%
  select(variable, coef = `coef`, HR = `exp(coef)`, p_value = `Pr(>|z|)`) %>%
  arrange(p_value)

# Print most significant predictors
print("Most significant predictors (p < 0.05):")
print(coef_df %>% filter(p_value < 0.05))

# 12. Variable importance plot
ggplot(coef_df, aes(x = reorder(variable, HR), y = HR)) +
  geom_point() +
  geom_errorbar(aes(ymin = HR * exp(-1.96 * `coef`), 
                    ymax = HR * exp(1.96 * `coef`)), 
                width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  coord_flip() +
  labs(x = "Variable", y = "Hazard Ratio (95% CI)", 
       title = "Variable Importance Plot") +
  theme_minimal()

# 13. Cumulative prediction accuracy across time
integrated_auc <- with(troc, cbind(times/365, AUC))
colnames(integrated_auc) <- c("Years", "AUC")
print("Integrated time-dependent AUC:")
print(integrated_auc)

# 14. Create Kaplan-Meier survival curves for a key variable
km_fit <- survfit(Surv(match_duration, event) ~ Program.Type, data = df_model)

# Plot survival curves
ggsurvplot(
  km_fit,
  data = df_model,
  risk.table = TRUE,
  pval = TRUE,
  conf.int = TRUE,
  xlim = c(0, 1000),
  xlab = "Days",
  ylab = "Match Survival Probability",
  title = "Match Survival by Program Type",
  ggtheme = theme_minimal()
)
```

